diff --git a/kernel-headers/CMakeLists.txt b/kernel-headers/CMakeLists.txt
index 46e2ef39..436dfb4d 100644
--- a/kernel-headers/CMakeLists.txt
+++ b/kernel-headers/CMakeLists.txt
@@ -2,6 +2,7 @@ publish_internal_headers(rdma
   rdma/bnxt_re-abi.h
   rdma/cxgb4-abi.h
   rdma/efa-abi.h
+  rdma/erdma-abi.h
   rdma/hns-abi.h
   rdma/ib_user_ioctl_cmds.h
   rdma/ib_user_ioctl_verbs.h
@@ -23,7 +24,6 @@ publish_internal_headers(rdma
   rdma/rdma_user_rxe.h
   rdma/rvt-abi.h
   rdma/siw-abi.h
-  rdma/erdma-abi.h
   rdma/vmw_pvrdma-abi.h
   )

@@ -66,6 +66,7 @@ rdma_kernel_provider_abi(
   rdma/bnxt_re-abi.h
   rdma/cxgb4-abi.h
   rdma/efa-abi.h
+  rdma/erdma-abi.h
   rdma/hns-abi.h
   rdma/ib_user_verbs.h
   rdma/irdma-abi.h
@@ -76,7 +77,6 @@ rdma_kernel_provider_abi(
   rdma/qedr-abi.h
   rdma/rdma_user_rxe.h
   rdma/siw-abi.h
-  rdma/erdma-abi.h
   rdma/vmw_pvrdma-abi.h
   )

diff --git a/kernel-headers/rdma/erdma-abi.h b/kernel-headers/rdma/erdma-abi.h
index e3ceef30..b7a0222f 100644
--- a/kernel-headers/rdma/erdma-abi.h
+++ b/kernel-headers/rdma/erdma-abi.h
@@ -1,6 +1,6 @@
-/* SPDX-License-Identifier: ((GPL-2.0 WITH Linux-syscall-note) OR Linux-OpenIB) */
+/* SPDX-License-Identifier: ((GPL-2.0 WITH Linux-syscall-note) OR BSD-3-Clause) */
 /*
- * Copyright (c) 2020-2021, Alibaba Group.
+ * Copyright (c) 2020-2022, Alibaba Group.
  */

 #ifndef __ERDMA_USER_H__
@@ -11,8 +11,8 @@
 #define ERDMA_ABI_VERSION       1

 struct erdma_ureq_create_cq {
-	__u64 db_record_va;
-	__u64 qbuf_va;
+	__aligned_u64 db_record_va;
+	__aligned_u64 qbuf_va;
 	__u32 qbuf_len;
 	__u32 rsvd0;
 };
@@ -23,8 +23,8 @@ struct erdma_uresp_create_cq {
 };

 struct erdma_ureq_create_qp {
-	__u64 db_record_va;
-	__u64 qbuf_va;
+	__aligned_u64 db_record_va;
+	__aligned_u64 qbuf_va;
 	__u32 qbuf_len;
 	__u32 rsvd0;
 };
@@ -41,9 +41,9 @@ struct erdma_uresp_alloc_ctx {
 	__u32 pad;
 	__u32 sdb_type;
 	__u32 sdb_offset;
-	__u64 sdb;
-	__u64 rdb;
-	__u64 cdb;
+	__aligned_u64 sdb;
+	__aligned_u64 rdb;
+	__aligned_u64 cdb;
 };

 #endif
diff --git a/providers/erdma/erdma.c b/providers/erdma/erdma.c
index 32f24c49..3a49490a 100644
--- a/providers/erdma/erdma.c
+++ b/providers/erdma/erdma.c
@@ -19,6 +19,7 @@

 static const struct verbs_context_ops erdma_context_ops = {
 	.alloc_pd = erdma_alloc_pd,
+	.cq_event = erdma_cq_event,
 	.create_cq = erdma_create_cq,
 	.create_qp = erdma_create_qp,
 	.dealloc_pd = erdma_free_pd,
@@ -27,7 +28,6 @@ static const struct verbs_context_ops erdma_context_ops = {
 	.destroy_qp = erdma_destroy_qp,
 	.free_context = erdma_free_context,
 	.modify_qp = erdma_modify_qp,
-	.cq_event = erdma_cq_event,
 	.poll_cq = erdma_poll_cq,
 	.post_recv = erdma_post_recv,
 	.post_send = erdma_post_send,
@@ -41,9 +41,9 @@ static const struct verbs_context_ops erdma_context_ops = {
 static struct verbs_context *erdma_alloc_context(struct ibv_device *device,
 						 int cmd_fd, void *private_data)
 {
-	struct erdma_context *ctx;
-	struct ibv_get_context cmd = {};
 	struct erdma_cmd_alloc_context_resp resp = {};
+	struct ibv_get_context cmd = {};
+	struct erdma_context *ctx;
 	int i;

 	ctx = verbs_init_and_alloc_context(device, cmd_fd, ctx, ibv_ctx,
@@ -57,7 +57,7 @@ static struct verbs_context *erdma_alloc_context(struct ibv_device *device,

 	if (ibv_cmd_get_context(&ctx->ibv_ctx, &cmd, sizeof(cmd),
 				&resp.ibv_resp, sizeof(resp)))
-		goto fail;
+		goto err_out;

 	verbs_set_ops(&ctx->ibv_ctx, &erdma_context_ops);
 	ctx->dev_id = resp.dev_id;
@@ -67,35 +67,33 @@ static struct verbs_context *erdma_alloc_context(struct ibv_device *device,

 	ctx->sdb = mmap(NULL, ERDMA_PAGE_SIZE, PROT_WRITE, MAP_SHARED, cmd_fd,
 			resp.sdb);
-	if (!ctx->sdb)
-		goto fail;
+	if (ctx->sdb == MAP_FAILED)
+		goto err_out;

 	ctx->rdb = mmap(NULL, ERDMA_PAGE_SIZE, PROT_WRITE, MAP_SHARED, cmd_fd,
 			resp.rdb);
-	if (!ctx->rdb)
-		goto fail;
+	if (ctx->rdb == MAP_FAILED)
+		goto err_rdb_map;

 	ctx->cdb = mmap(NULL, ERDMA_PAGE_SIZE, PROT_WRITE, MAP_SHARED, cmd_fd,
 			resp.cdb);
-	if (!ctx->cdb)
-		goto fail;
+	if (ctx->cdb == MAP_FAILED)
+		goto err_cdb_map;

 	ctx->page_size = ERDMA_PAGE_SIZE;
-	ctx->dbrecord_pages = NULL;
+	list_head_init(&ctx->dbrecord_pages_list);
 	pthread_mutex_init(&ctx->dbrecord_pages_mutex, NULL);

 	return &ctx->ibv_ctx;

-fail:
-	if (ctx->sdb)
-		munmap(ctx->sdb, ERDMA_PAGE_SIZE);
-	if (ctx->rdb)
-		munmap(ctx->rdb, ERDMA_PAGE_SIZE);
-	if (ctx->cdb)
-		munmap(ctx->cdb, ERDMA_PAGE_SIZE);
-
+err_cdb_map:
+	munmap(ctx->rdb, ERDMA_PAGE_SIZE);
+err_rdb_map:
+	munmap(ctx->sdb, ERDMA_PAGE_SIZE);
+err_out:
 	verbs_uninit_context(&ctx->ibv_ctx);
 	free(ctx);
+
 	return NULL;
 }

@@ -122,7 +120,6 @@ static void erdma_device_free(struct verbs_device *vdev)
 static const struct verbs_match_ent match_table[] = {
 	VERBS_DRIVER_ID(RDMA_DRIVER_ERDMA),
 	VERBS_PCI_MATCH(PCI_VENDOR_ID_ALIBABA, 0x107f, NULL),
-	VERBS_PCI_MATCH(PCI_VENDOR_ID_ALIBABA, 0x5007, NULL),
 	{},
 };

diff --git a/providers/erdma/erdma.h b/providers/erdma/erdma.h
index 512c6ef1..ce13a8b7 100644
--- a/providers/erdma/erdma.h
+++ b/providers/erdma/erdma.h
@@ -7,21 +7,19 @@
 #ifndef __ERDMA_H__
 #define __ERDMA_H__

-#include <pthread.h>
 #include <inttypes.h>
+#include <pthread.h>
 #include <stddef.h>

 #include <infiniband/driver.h>
 #include <infiniband/kern-abi.h>
+#include <sys/param.h>

 #ifndef PCI_VENDOR_ID_ALIBABA
 #define PCI_VENDOR_ID_ALIBABA 0x1ded
 #endif

 #define ERDMA_PAGE_SIZE 4096
-#define ERDMA_PAGE_SHIFT 12
-#define ERDMA_SIZE_TO_NPAGE(size)                                              \
-	(((size) + ERDMA_PAGE_SIZE - 1) >> ERDMA_PAGE_SHIFT)

 struct erdma_device {
 	struct verbs_device ibv_dev;
@@ -48,9 +46,9 @@ struct erdma_context {
 	void *rdb;
 	void *cdb;

-	int page_size;
+	uint32_t page_size;
 	pthread_mutex_t dbrecord_pages_mutex;
-	struct erdma_dbrecord_page *dbrecord_pages;
+	struct list_head dbrecord_pages_list;
 };

 static inline struct erdma_context *to_ectx(struct ibv_context *base)
diff --git a/providers/erdma/erdma_db.c b/providers/erdma/erdma_db.c
index 382262da..503b6ccf 100644
--- a/providers/erdma/erdma_db.c
+++ b/providers/erdma/erdma_db.c
@@ -3,110 +3,103 @@
 // Authors: Cheng Xu <chengyou@linux.alibaba.com>
 // Copyright (c) 2020-2021, Alibaba Group.

-// Copyright (c) 2012 Mellanox Technologies, Inc.  All rights reserved.
-
-#define _GNU_SOURCE
 #include <inttypes.h>
 #include <stdlib.h>
 #include <string.h>
+#include <util/bitmap.h>
 #include <util/util.h>

 #include "erdma.h"
 #include "erdma_db.h"

-#define ERDMA_DBRECORDS_SIZE 16
+#define ERDMA_DBREC_SIZE 16

 struct erdma_dbrecord_page {
-	struct erdma_dbrecord_page *prev, *next;
+	struct list_node list;
 	void *page_buf;
-	int cnt;
-	int used;
-	unsigned long free[0];
+	uint32_t cnt;
+	uint32_t used;
+	unsigned long *bitmap;
 };

 uint64_t *erdma_alloc_dbrecords(struct erdma_context *ctx)
 {
-	int bits_perlong = (8 * sizeof(unsigned long));
 	struct erdma_dbrecord_page *page = NULL;
-	int dbrecords_per_page, nlongs = 0;
+	uint32_t free_idx, dbrecords_per_page;
 	uint64_t *db_records = NULL;
-	int i, j, rv;
+	int rv;

 	pthread_mutex_lock(&ctx->dbrecord_pages_mutex);

-	for (page = ctx->dbrecord_pages; page; page = page->next)
+	list_for_each(&ctx->dbrecord_pages_list, page, list)
 		if (page->used < page->cnt)
 			goto found;

-	dbrecords_per_page = ctx->page_size / ERDMA_DBRECORDS_SIZE;
-	nlongs = align(dbrecords_per_page, bits_perlong) / bits_perlong;
-	page = malloc(sizeof(*page) + nlongs * sizeof(unsigned long));
+	dbrecords_per_page = ctx->page_size / ERDMA_DBREC_SIZE;
+
+	page = calloc(1, sizeof(*page));
 	if (!page)
-		goto out;
+		goto err_out;
+
+	page->bitmap = bitmap_alloc1(dbrecords_per_page);
+	if (!page->bitmap)
+		goto err_bitmap;

 	rv = posix_memalign(&page->page_buf, ctx->page_size, ctx->page_size);
-	if (rv) {
-		free(page);
-		goto out;
-	}
+	if (rv)
+		goto err_alloc;

 	page->cnt = dbrecords_per_page;
 	page->used = 0;
-	for (i = 0; i < nlongs; i++)
-		page->free[i] = ~0UL;

-	page->prev = NULL;
-	page->next = ctx->dbrecord_pages;
-	ctx->dbrecord_pages = page;
-	if (page->next)
-		page->next->prev = page;
+	list_node_init(&page->list);
+	list_add_tail(&ctx->dbrecord_pages_list, &page->list);

 found:
 	++page->used;

-	for (i = 0; !page->free[i]; ++i)
-		; /* nothing */
-
-	j = ffsl(page->free[i]) - 1;
-	page->free[i] &= ~(1UL << j);
+	free_idx = bitmap_find_first_bit(page->bitmap, 0, page->cnt);
+	bitmap_clear_bit(page->bitmap, free_idx);

-	db_records =
-		page->page_buf + (i * bits_perlong + j) * ERDMA_DBRECORDS_SIZE;
+	db_records = page->page_buf + free_idx * ERDMA_DBREC_SIZE;

-out:
 	pthread_mutex_unlock(&ctx->dbrecord_pages_mutex);

 	return db_records;
+
+err_alloc:
+	free(page->bitmap);
+err_bitmap:
+	free(page);
+err_out:
+	pthread_mutex_unlock(&ctx->dbrecord_pages_mutex);
+
+	return NULL;
 }

-void erdma_dealloc_dbrecords(struct erdma_context *ctx, uint64_t *dbrecords)
+void erdma_dealloc_dbrecords(struct erdma_context *ctx, uint64_t *dbrec)
 {
+	uint32_t page_mask = ~(ctx->page_size - 1);
 	struct erdma_dbrecord_page *page;
-	int page_mask = ~(ctx->page_size - 1);
-	int idx;
+	uint32_t idx;

 	pthread_mutex_lock(&ctx->dbrecord_pages_mutex);
-	for (page = ctx->dbrecord_pages; page; page = page->next)
-		if (((uintptr_t)dbrecords & page_mask) ==
-		    (uintptr_t)page->page_buf)
-			break;

-	if (!page)
-		goto out;
+	list_for_each(&ctx->dbrecord_pages_list, page, list)
+		if (((uintptr_t)dbrec & page_mask) == (uintptr_t)page->page_buf)
+			goto found;
+
+	goto out;

-	idx = ((void *)dbrecords - page->page_buf) / ERDMA_DBRECORDS_SIZE;
-	page->free[idx / (8 * sizeof(unsigned long))] |=
-		1UL << (idx % (8 * sizeof(unsigned long)));
+found:
+	idx = ((uintptr_t)dbrec - (uintptr_t)page->page_buf) / ERDMA_DBREC_SIZE;

-	if (!--page->used) {
-		if (page->prev)
-			page->prev->next = page->next;
-		else
-			ctx->dbrecord_pages = page->next;
-		if (page->next)
-			page->next->prev = page->prev;
+	bitmap_set_bit(page->bitmap, idx);

-		free(page->page_buf);
+	page->used--;
+	if (!page->used) {
+		list_del(&page->list);
+		free(page->bitmap);
 		free(page);
 	}

diff --git a/providers/erdma/erdma_verbs.c b/providers/erdma/erdma_verbs.c
index 5acfa8b8..6930e388 100644
--- a/providers/erdma/erdma_verbs.c
+++ b/providers/erdma/erdma_verbs.c
@@ -27,9 +27,9 @@ int erdma_query_device(struct ibv_context *ctx,
 		       struct ibv_device_attr_ex *attr, size_t attr_size)
 {
 	struct ib_uverbs_ex_query_device_resp resp;
+	unsigned int major, minor, sub_minor;
 	size_t resp_size = sizeof(resp);
 	uint64_t raw_fw_ver;
-	unsigned int major, minor, sub_minor;
 	int rv;

 	rv = ibv_cmd_query_device_any(ctx, input, attr, attr_size, &resp,
@@ -67,8 +67,8 @@ int erdma_query_qp(struct ibv_qp *qp, struct ibv_qp_attr *attr, int attr_mask,

 struct ibv_pd *erdma_alloc_pd(struct ibv_context *ctx)
 {
-	struct ibv_alloc_pd cmd = {};
 	struct ib_uverbs_alloc_pd_resp resp;
+	struct ibv_alloc_pd cmd = {};
 	struct ibv_pd *pd;

 	pd = calloc(1, sizeof(*pd));
@@ -98,8 +98,8 @@ int erdma_free_pd(struct ibv_pd *pd)
 struct ibv_mr *erdma_reg_mr(struct ibv_pd *pd, void *addr, size_t len,
 			    uint64_t hca_va, int access)
 {
-	struct ibv_reg_mr cmd;
 	struct ib_uverbs_reg_mr_resp resp;
+	struct ibv_reg_mr cmd;
 	struct verbs_mr *vmr;
 	int ret;

@@ -160,16 +160,14 @@ struct ibv_cq *erdma_create_cq(struct ibv_context *ctx, int num_cqe,
 			       struct ibv_comp_channel *channel,
 			       int comp_vector)
 {
-	struct erdma_context *ectx;
-	struct erdma_cmd_create_cq cmd = {};
+	struct erdma_context *ectx = to_ectx(ctx);
 	struct erdma_cmd_create_cq_resp resp = {};
-	struct erdma_cq *cq;
+	struct erdma_cmd_create_cq cmd = {};
 	uint64_t *db_records = NULL;
-	int cq_size;
+	struct erdma_cq *cq;
+	size_t cq_size;
 	int rv;

-	ectx = to_ectx(ctx);
-
 	cq = calloc(1, sizeof(*cq));
 	if (!cq)
 		return NULL;
@@ -178,8 +176,7 @@ struct ibv_cq *erdma_create_cq(struct ibv_context *ctx, int num_cqe,
 		num_cqe = 64;

 	num_cqe = roundup_pow_of_two(num_cqe);
-	cq_size = num_cqe * sizeof(struct erdma_cqe);
-	cq_size = ERDMA_SIZE_TO_NPAGE(cq_size) << ERDMA_PAGE_SHIFT;
+	cq_size = align(num_cqe * sizeof(struct erdma_cqe), ERDMA_PAGE_SIZE);

 	rv = posix_memalign((void **)&cq->queue, ERDMA_PAGE_SIZE, cq_size);
 	if (rv) {
@@ -215,7 +212,6 @@ struct ibv_cq *erdma_create_cq(struct ibv_context *ctx, int num_cqe,

 	cq->id = resp.cq_id;
 	cq->depth = resp.num_cqe;
-	cq->owner = 1;

 	cq->db = ectx->cdb;
 	cq->db_offset = (cq->id & (ERDMA_PAGE_SIZE / ERDMA_CQDB_SIZE - 1)) *
@@ -240,8 +236,8 @@ error_alloc:

 int erdma_destroy_cq(struct ibv_cq *base_cq)
 {
-	struct erdma_cq *cq = to_ecq(base_cq);
 	struct erdma_context *ctx = to_ectx(base_cq->context);
+	struct erdma_cq *cq = to_ecq(base_cq);
 	int rv;

 	pthread_spin_lock(&cq->lock);
@@ -283,28 +279,28 @@ static void __erdma_alloc_dbs(struct erdma_qp *qp, struct erdma_context *ctx)

 struct ibv_qp *erdma_create_qp(struct ibv_pd *pd, struct ibv_qp_init_attr *attr)
 {
-	struct erdma_cmd_create_qp cmd = {};
-	struct erdma_cmd_create_qp_resp resp = {};
-	struct erdma_qp *qp;
 	struct ibv_context *base_ctx = pd->context;
 	struct erdma_context *ctx = to_ectx(base_ctx);
+	struct erdma_cmd_create_qp_resp resp = {};
+	struct erdma_cmd_create_qp cmd = {};
+	uint32_t tbl_idx, tbl_off, nsqebb;
 	uint64_t *db_records = NULL;
-	int rv, tbl_idx, tbl_off;
-	int sq_size = 0, rq_size = 0, total_bufsize = 0;
+	struct erdma_qp *qp;
+	size_t queue_size;
+	int rv;

 	qp = calloc(1, sizeof(*qp));
 	if (!qp)
 		return NULL;

-	sq_size = roundup_pow_of_two(attr->cap.max_send_wr * MAX_WQEBB_PER_SQE)
-		  << SQEBB_SHIFT;
-	sq_size = align(sq_size, ctx->page_size);
-	rq_size = align(roundup_pow_of_two(attr->cap.max_recv_wr) << RQE_SHIFT,
-			ctx->page_size);
-	total_bufsize = sq_size + rq_size;
-	rv = posix_memalign(&qp->qbuf, ctx->page_size, total_bufsize);
-	if (rv || !qp->qbuf) {
-		errno = ENOMEM;
+	nsqebb = roundup_pow_of_two(attr->cap.max_send_wr * MAX_WQEBB_PER_SQE);
+	queue_size = align(nsqebb << SQEBB_SHIFT, ctx->page_size);
+	queue_size +=
+		align(roundup_pow_of_two(attr->cap.max_recv_wr) << RQE_SHIFT,
+		      ctx->page_size);
+	rv = posix_memalign(&qp->qbuf, ctx->page_size, queue_size);
+	if (rv) {
+		errno = rv;
 		goto error_alloc;
 	}

@@ -316,14 +312,12 @@ struct ibv_qp *erdma_create_qp(struct ibv_pd *pd, struct ibv_qp_init_attr *attr)

 	cmd.db_record_va = (uintptr_t)db_records;
 	cmd.qbuf_va = (uintptr_t)qp->qbuf;
-	cmd.qbuf_len = (__u32)total_bufsize;
+	cmd.qbuf_len = (__u32)queue_size;

 	rv = ibv_cmd_create_qp(pd, &qp->base_qp, attr, &cmd.ibv_cmd,
 			       sizeof(cmd), &resp.ibv_resp, sizeof(resp));
-	if (rv) {
-		errno = EIO;
+	if (rv)
 		goto error_alloc;
-	}

 	qp->id = resp.qp_id;

@@ -333,7 +327,7 @@ struct ibv_qp *erdma_create_qp(struct ibv_pd *pd, struct ibv_qp_init_attr *attr)

 	if (ctx->qp_table[tbl_idx].refcnt == 0) {
 		ctx->qp_table[tbl_idx].table =
-			calloc(ERDMA_PAGE_SIZE, sizeof(struct erdma_qp *));
+			calloc(ERDMA_QP_TABLE_SIZE, sizeof(struct erdma_qp *));
 		if (!ctx->qp_table[tbl_idx].table) {
 			errno = ENOMEM;
 			goto fail;
@@ -403,8 +397,8 @@ error_alloc:
 int erdma_modify_qp(struct ibv_qp *base_qp, struct ibv_qp_attr *attr,
 		    int attr_mask)
 {
-	struct ibv_modify_qp cmd = {};
 	struct erdma_qp *qp = to_eqp(base_qp);
+	struct ibv_modify_qp cmd = {};
 	int rv;

 	pthread_spin_lock(&qp->sq_lock);
@@ -420,10 +414,11 @@ int erdma_modify_qp(struct ibv_qp *base_qp, struct ibv_qp_attr *attr,

 int erdma_destroy_qp(struct ibv_qp *base_qp)
 {
-	struct erdma_qp *qp = to_eqp(base_qp);
 	struct ibv_context *base_ctx = base_qp->pd->context;
 	struct erdma_context *ctx = to_ectx(base_ctx);
-	int rv, tbl_idx, tbl_off;
+	struct erdma_qp *qp = to_eqp(base_qp);
+	uint32_t tbl_idx, tbl_off;
+	int rv;

 	pthread_mutex_lock(&ctx->qp_table_mutex);
 	tbl_idx = qp->id >> ERDMA_QP_TABLE_SHIFT;
@@ -460,17 +455,15 @@ int erdma_destroy_qp(struct ibv_qp *base_qp)
 static int erdma_push_one_sqe(struct erdma_qp *qp, struct ibv_send_wr *wr,
 			      uint16_t *sq_pi)
 {
-	uint16_t tmp_pi = *sq_pi;
-	void *sqe;
-	uint64_t sqe_hdr;
+	uint32_t i, bytes, sgl_off, sgl_idx, wqebb_cnt, opcode, wqe_size = 0;
+	struct erdma_readreq_sqe *read_sqe;
 	struct erdma_write_sqe *write_sqe;
 	struct erdma_send_sqe *send_sqe;
-	struct erdma_readreq_sqe *read_sqe;
-	uint32_t wqe_size = 0;
-	__le32 *length_field = NULL;
-	struct erdma_sge *sgl_base = NULL;
-	uint32_t i, bytes = 0;
-	uint32_t sgl_off, sgl_idx, wqebb_cnt, opcode;
+	struct erdma_sge *sgl_base;
+	uint16_t tmp_pi = *sq_pi;
+	__le32 *length_field;
+	uint64_t sqe_hdr;
+	void *sqe;

 	sqe = get_sq_wqebb(qp, tmp_pi);
 	/* Clear the first 8Byte of the wqe hdr. */
@@ -497,7 +490,7 @@ static int erdma_push_one_sqe(struct erdma_qp *qp, struct ibv_send_wr *wr,
 		else
 			opcode = ERDMA_OP_WRITE_WITH_IMM;
 		sqe_hdr |= FIELD_PREP(ERDMA_SQE_HDR_OPCODE_MASK, opcode);
-		write_sqe = (struct erdma_write_sqe *)sqe;
+		write_sqe = sqe;
 		write_sqe->imm_data = wr->imm_data;
 		write_sqe->sink_stag = htole32(wr->wr.rdma.rkey);
 		write_sqe->sink_to_low =
@@ -506,8 +499,8 @@ static int erdma_push_one_sqe(struct erdma_qp *qp, struct ibv_send_wr *wr,
 			htole32((wr->wr.rdma.remote_addr >> 32) & 0xFFFFFFFF);

 		length_field = &write_sqe->length;
+		/* sgl is at the start of next wqebb. */
 		sgl_base = get_sq_wqebb(qp, tmp_pi + 1);
-		/* sgl is in next wqebb. */
 		sgl_off = 0;
 		sgl_idx = tmp_pi + 1;
 		wqe_size = sizeof(struct erdma_write_sqe);
@@ -520,12 +513,12 @@ static int erdma_push_one_sqe(struct erdma_qp *qp, struct ibv_send_wr *wr,
 		else
 			opcode = ERDMA_OP_SEND_WITH_IMM;
 		sqe_hdr |= FIELD_PREP(ERDMA_SQE_HDR_OPCODE_MASK, opcode);
-		send_sqe = (struct erdma_send_sqe *)sqe;
+		send_sqe = sqe;
 		send_sqe->imm_data = wr->imm_data;

 		length_field = &send_sqe->length;
-		sgl_base = (void *)send_sqe;
-		/* sgl is in the half of current wqebb */
+		/* sgl is in the half of current wqebb (offset 16Byte) */
+		sgl_base = sqe;
 		sgl_off = 16;
 		sgl_idx = tmp_pi;
 		wqe_size = sizeof(struct erdma_send_sqe);
@@ -533,8 +526,7 @@ static int erdma_push_one_sqe(struct erdma_qp *qp, struct ibv_send_wr *wr,
 		break;
 	case IBV_WR_RDMA_READ:
 		sqe_hdr |= FIELD_PREP(ERDMA_SQE_HDR_OPCODE_MASK, ERDMA_OP_READ);
-
-		read_sqe = (struct erdma_readreq_sqe *)sqe;
+		read_sqe = sqe;

 		read_sqe->sink_to_low = htole32(wr->sg_list->addr & 0xFFFFFFFF);
 		read_sqe->sink_to_high =
@@ -542,12 +534,11 @@ static int erdma_push_one_sqe(struct erdma_qp *qp, struct ibv_send_wr *wr,
 		read_sqe->sink_stag = htole32(wr->sg_list->lkey);
 		read_sqe->length = htole32(wr->sg_list->length);

-		struct erdma_sge *sgl =
-			(struct erdma_sge *)get_sq_wqebb(qp, tmp_pi + 1);
+		sgl_base = get_sq_wqebb(qp, tmp_pi + 1);

-		sgl->laddr = htole64(wr->wr.rdma.remote_addr);
-		sgl->length = htole32(wr->sg_list->length);
-		sgl->lkey = htole32(wr->wr.rdma.rkey);
+		sgl_base->laddr = htole64(wr->wr.rdma.remote_addr);
+		sgl_base->length = htole32(wr->sg_list->length);
+		sgl_base->lkey = htole32(wr->wr.rdma.rkey);

 		wqe_size = sizeof(struct erdma_readreq_sqe);

@@ -647,8 +638,8 @@ int erdma_post_send(struct ibv_qp *base_qp, struct ibv_send_wr *wr,
 		    struct ibv_send_wr **bad_wr)
 {
 	struct erdma_qp *qp = to_eqp(base_qp);
-	uint16_t sq_pi;
 	int new_sqe = 0, rv = 0;
+	uint16_t sq_pi;

 	*bad_wr = NULL;

@@ -755,6 +746,14 @@ void erdma_cq_event(struct ibv_cq *ibcq)
 	cq->cmdsn++;
 }

+static void *get_next_valid_cqe(struct erdma_cq *cq)
+{
+	struct erdma_cqe *cqe = cq->queue + (cq->ci & (cq->depth - 1));
+	uint32_t owner = FIELD_GET(ERDMA_CQE_HDR_OWNER_MASK, be32toh(cqe->hdr));
+
+	return owner ^ !!(cq->ci & cq->depth) ? cqe : NULL;
+}
+
 static const struct {
 	enum erdma_opcode erdma;
 	enum ibv_wc_opcode base;
@@ -821,21 +820,30 @@ static const struct {
 #define ERDMA_POLLCQ_WRONG_IDX (-3)

 static int __erdma_poll_one_cqe(struct erdma_context *ctx, struct erdma_cq *cq,
-				struct erdma_cqe *cqe, uint32_t cqe_hdr,
 				struct ibv_wc *wc)
 {
+	uint32_t cqe_hdr, opcode, syndrome, qpn;
+	uint16_t depth, wqe_idx, old_ci, new_ci;
+	uint64_t *sqe_hdr, *qeidx2wrid;
+	uint32_t tbl_idx, tbl_off;
+	struct erdma_cqe *cqe;
 	struct erdma_qp *qp;
-	uint64_t *qeidx2wrid = NULL;
-	uint32_t qpn = be32toh(cqe->qpn);
-	uint16_t depth = 0;
-	uint64_t *sqe_hdr;
-	uint16_t wqe_idx = be32toh(cqe->qe_idx);
-	uint16_t old_ci, new_ci;
-	uint32_t opcode = FIELD_GET(ERDMA_CQE_HDR_OPCODE_MASK, cqe_hdr);
-	uint32_t syndrome = FIELD_GET(ERDMA_CQE_HDR_SYNDROME_MASK, cqe_hdr);
-
-	int tbl_idx = qpn >> ERDMA_QP_TABLE_SHIFT;
-	int tbl_off = qpn & ERDMA_QP_TABLE_MASK;
+
+	cqe = get_next_valid_cqe(cq);
+	if (!cqe)
+		return -EAGAIN;
+
+	cq->ci++;
+	udma_from_device_barrier();
+
+	cqe_hdr = be32toh(cqe->hdr);
+	syndrome = FIELD_GET(ERDMA_CQE_HDR_SYNDROME_MASK, cqe_hdr);
+	opcode = FIELD_GET(ERDMA_CQE_HDR_OPCODE_MASK, cqe_hdr);
+	qpn = be32toh(cqe->qpn);
+	wqe_idx = be32toh(cqe->qe_idx);
+
+	tbl_idx = qpn >> ERDMA_QP_TABLE_SHIFT;
+	tbl_off = qpn & ERDMA_QP_TABLE_MASK;

 	if (!ctx->qp_table[tbl_idx].table ||
 	    !ctx->qp_table[tbl_idx].table[tbl_off])
@@ -847,7 +855,7 @@ static int __erdma_poll_one_cqe(struct erdma_context *ctx, struct erdma_cq *cq,
 	    ERDMA_CQE_QTYPE_SQ) {
 		qeidx2wrid = qp->sq.wr_tbl;
 		depth = qp->sq.depth;
-		sqe_hdr = (uint64_t *)get_sq_wqebb(qp, wqe_idx);
+		sqe_hdr = get_sq_wqebb(qp, wqe_idx);
 		old_ci = qp->sq.ci;
 		new_ci = wqe_idx +
 			 FIELD_GET(ERDMA_SQE_HDR_WQEBB_CNT_MASK, *sqe_hdr) + 1;
@@ -892,49 +900,24 @@ static int __erdma_poll_one_cqe(struct erdma_context *ctx, struct erdma_cq *cq,

 int erdma_poll_cq(struct ibv_cq *ibcq, int num_entries, struct ibv_wc *wc)
 {
-	struct erdma_cq *cq = to_ecq(ibcq);
 	struct erdma_context *ctx = to_ectx(ibcq->context);
-	int new = 0;
-	struct erdma_cqe *cqe;
-	int owner;
-	uint32_t ci;
-	uint32_t depth_mask = cq->depth - 1;
-	uint32_t hdr;
-	int i, ret;
+	struct erdma_cq *cq = to_ecq(ibcq);
+	int ret, npolled = 0;

 	pthread_spin_lock(&cq->lock);

-	owner = cq->owner;
-	ci = cq->ci;
-
-	for (i = 0; i < num_entries; i++) {
-		cqe = &cq->queue[ci & depth_mask];
-		hdr = be32toh(cqe->hdr);
-
-		if (FIELD_GET(ERDMA_CQE_HDR_OWNER_MASK, hdr) != owner)
+	while (npolled < num_entries) {
+		ret = __erdma_poll_one_cqe(ctx, cq, wc + npolled);
+		if (ret == -EAGAIN) /* CQ is empty, break the loop. */
 			break;
-
-		udma_from_device_barrier();
-
-		ret = __erdma_poll_one_cqe(ctx, cq, cqe, hdr, wc);
-
-		ci++;
-		if ((ci & depth_mask) == 0)
-			owner = !owner;
-
-		if (ret)
+		else if (ret) /* We handle the polling error silently. */
 			continue;
-
-		wc++;
-		new++;
+		npolled++;
 	}

-	cq->owner = owner;
-	cq->ci = ci;
-
 	pthread_spin_unlock(&cq->lock);

-	return new;
+	return npolled;
 }

 void erdma_free_context(struct ibv_context *ibv_ctx)
diff --git a/providers/erdma/erdma_verbs.h b/providers/erdma/erdma_verbs.h
index fd83f049..083126d9 100644
--- a/providers/erdma/erdma_verbs.h
+++ b/providers/erdma/erdma_verbs.h
@@ -61,7 +61,6 @@ struct erdma_cq {

 	uint32_t depth;
 	uint32_t ci;
-	uint32_t owner;
 	struct erdma_cqe *queue;

 	void *db;
@@ -69,8 +68,8 @@ struct erdma_cq {

 	void *db_record;
 	uint32_t cmdsn;
-	int comp_vector;
-	int db_index;
+	uint32_t comp_vector;
+	uint32_t db_index;

 	pthread_spinlock_t lock;
 };
